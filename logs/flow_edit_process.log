2024/11/01 19:59:18 - [INFO] Load FlowEditPipeline from `/data/pengzhengwei/checkpoints/stablediffusion/v3` successfully, inferencing with `torch.float16` on `cuda:5`.
2024/11/01 19:59:18 - [DEBUG] STREAM b'IHDR' 16 13
2024/11/01 19:59:18 - [DEBUG] STREAM b'sRGB' 41 1
2024/11/01 19:59:18 - [DEBUG] STREAM b'gAMA' 54 4
2024/11/01 19:59:18 - [DEBUG] STREAM b'pHYs' 70 9
2024/11/01 19:59:18 - [DEBUG] STREAM b'IDAT' 91 65445
2024/11/01 19:59:18 - [INFO] Set random seed `1234`.
2024/11/01 19:59:18 - [INFO] Set step end callback `save_inter_latents_callback`
2024/11/01 19:59:19 - [INFO] =========== Initializing AttentionRefine controller. ===========
2024/11/01 19:59:19 - [INFO] source prompt is `the apple is on the desk`.
2024/11/01 19:59:19 - [INFO] target prompt is `the banana is on the desk`.
2024/11/01 19:59:19 - [INFO] source blend is ``.
2024/11/01 19:59:19 - [INFO] target blend is ``.
2024/11/01 19:59:19 - [INFO] source encoded sequence is `[['<|startoftext|>', 'the', 'apple', '-', 'is', 'on', 'the', 'desk', '<|endoftext|>']]`.
2024/11/01 19:59:19 - [INFO] target encoded sequence is `[['<|startoftext|>', 'the', '-', 'banana', 'is', 'on', 'the', 'desk', '<|endoftext|>']]`.
2024/11/01 19:59:19 - [INFO] original mapper is tensor([[ 0,  1, -1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
         72, 73, 74, 75, 76]]).
2024/11/01 19:59:19 - [INFO] final mapper is `tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
         72, 73, 74, 75, 76]])`.
2024/11/01 19:59:19 - [INFO] final alphas is `tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1.]])`.
2024/11/01 19:59:19 - [INFO] total inference steps is `4`.
2024/11/01 19:59:19 - [INFO] attention replacement start step is 0.0.
2024/11/01 19:59:19 - [INFO] attention replacement end step is 1.0.
2024/11/01 19:59:19 - [INFO] visualize attention mode is `None`.
2024/11/01 19:59:19 - [INFO] set visualize attention now to `True`.
2024/11/01 19:59:19 - [INFO] extra args are `{'index': 2}`.
2024/11/01 19:59:19 - [INFO] ================================================================
2024/11/01 19:59:19 - [INFO] Registered 24 AttentionRefine controllers, namely ['transformer_blocks.0.attn.processor', 'transformer_blocks.1.attn.processor', 'transformer_blocks.2.attn.processor', 'transformer_blocks.3.attn.processor', 'transformer_blocks.4.attn.processor', 'transformer_blocks.5.attn.processor', 'transformer_blocks.6.attn.processor', 'transformer_blocks.7.attn.processor', 'transformer_blocks.8.attn.processor', 'transformer_blocks.9.attn.processor', 'transformer_blocks.10.attn.processor', 'transformer_blocks.11.attn.processor', 'transformer_blocks.12.attn.processor', 'transformer_blocks.13.attn.processor', 'transformer_blocks.14.attn.processor', 'transformer_blocks.15.attn.processor', 'transformer_blocks.16.attn.processor', 'transformer_blocks.17.attn.processor', 'transformer_blocks.18.attn.processor', 'transformer_blocks.19.attn.processor', 'transformer_blocks.20.attn.processor', 'transformer_blocks.21.attn.processor', 'transformer_blocks.22.attn.processor', 'transformer_blocks.23.attn.processor'].
2024/11/01 19:59:19 - [INFO] Replace attention at 0th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 1th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 2th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 3th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 4th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 5th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 6th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 7th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 8th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 9th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 10th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 11th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 12th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 13th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 14th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 15th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 16th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 17th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 18th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 19th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 20th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 21th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 22th layer.
2024/11/01 19:59:19 - [INFO] Replace attention at 23th layer.
2024/11/01 19:59:19 - [INFO] Complete the 1/4 step.
2024/11/01 19:59:20 - [INFO] Replace attention at 0th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 1th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 2th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 3th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 4th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 5th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 6th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 7th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 8th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 9th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 10th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 11th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 12th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 13th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 14th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 15th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 16th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 17th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 18th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 19th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 20th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 21th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 22th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 23th layer.
2024/11/01 19:59:20 - [INFO] Complete the 2/4 step.
2024/11/01 19:59:20 - [INFO] Replace attention at 0th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 1th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 2th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 3th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 4th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 5th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 6th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 7th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 8th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 9th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 10th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 11th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 12th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 13th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 14th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 15th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 16th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 17th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 18th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 19th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 20th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 21th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 22th layer.
2024/11/01 19:59:20 - [INFO] Replace attention at 23th layer.
2024/11/01 19:59:20 - [INFO] Complete the 3/4 step.
2024/11/01 19:59:21 - [INFO] Replace attention at 0th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 1th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 2th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 3th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 4th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 5th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 6th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 7th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 8th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 9th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 10th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 11th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 12th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 13th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 14th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 15th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 16th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 17th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 18th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 19th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 20th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 21th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 22th layer.
2024/11/01 19:59:21 - [INFO] Replace attention at 23th layer.
2024/11/01 19:59:21 - [INFO] Complete the 4/4 step.
