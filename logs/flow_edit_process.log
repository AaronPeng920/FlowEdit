2024/11/02 23:55:00 - [INFO] Load FlowEditPipeline from `/data/pengzhengwei/checkpoints/stablediffusion/v3` successfully, inferencing with `torch.float16` on `cuda:5`.
2024/11/02 23:55:00 - [INFO] Set random seed `1234`.
2024/11/02 23:55:00 - [INFO] Set step end callback `save_inter_latents_callback`
2024/11/02 23:55:00 - [INFO] =========== Initializing AttentionRefine controller. ===========
2024/11/02 23:55:00 - [INFO] source prompt is `a little carton sheep in a white background`.
2024/11/02 23:55:00 - [INFO] target prompt is `a little carton sheep in a forest background`.
2024/11/02 23:55:00 - [INFO] source blend is ``.
2024/11/02 23:55:00 - [INFO] target blend is ``.
2024/11/02 23:55:00 - [INFO] source encoded sequence is `[['<|startoftext|>', 'a', 'little', 'carton', 'sheep', 'in', 'a', 'white', '-', 'background', '<|endoftext|>']]`.
2024/11/02 23:55:00 - [INFO] target encoded sequence is `[['<|startoftext|>', 'a', 'little', 'carton', 'sheep', 'in', 'a', '-', 'forest', 'background', '<|endoftext|>']]`.
2024/11/02 23:55:00 - [INFO] original mapper is tensor([[  0,   1,   2,   3,   4,   5,   6,  -1,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]]).
2024/11/02 23:55:00 - [INFO] final mapper is `tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153]])`.
2024/11/02 23:55:00 - [INFO] final alphas is `tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])`.
2024/11/02 23:55:00 - [INFO] total inference steps is `10`.
2024/11/02 23:55:00 - [INFO] cross attention replacement start step is 0.0.
2024/11/02 23:55:00 - [INFO] cross attention replacement end step is 0.2.
2024/11/02 23:55:00 - [INFO] self attention replacement start step is 0.8.
2024/11/02 23:55:00 - [INFO] self attention replacement end step is 1.0.
2024/11/02 23:55:00 - [INFO] visualize attention mode is `None`.
2024/11/02 23:55:00 - [INFO] set visualize attention now to `True`.
2024/11/02 23:55:00 - [INFO] extra args are `{'index': 2}`.
2024/11/02 23:55:00 - [INFO] ================================================================
2024/11/02 23:55:01 - [INFO] Registered 9 AttentionRefine controllers, namely ['transformer_blocks.0.attn.processor', 'transformer_blocks.1.attn.processor', 'transformer_blocks.2.attn.processor', 'transformer_blocks.3.attn.processor', 'transformer_blocks.4.attn.processor', 'transformer_blocks.5.attn.processor', 'transformer_blocks.6.attn.processor', 'transformer_blocks.7.attn.processor', 'transformer_blocks.8.attn.processor'].
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 0th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 1th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 2th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 3th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 4th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 5th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 6th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 7th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 8th layer.
2024/11/02 23:55:01 - [INFO] Complete the 1/10 step.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 0th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 1th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 2th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 3th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 4th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 5th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 6th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 7th layer.
2024/11/02 23:55:01 - [INFO] Replace cross attention map at 8th layer.
2024/11/02 23:55:01 - [INFO] Complete the 2/10 step.
2024/11/02 23:55:02 - [INFO] Complete the 3/10 step.
2024/11/02 23:55:02 - [INFO] Complete the 4/10 step.
2024/11/02 23:55:03 - [INFO] Complete the 5/10 step.
2024/11/02 23:55:03 - [INFO] Complete the 6/10 step.
2024/11/02 23:55:04 - [INFO] Complete the 7/10 step.
2024/11/02 23:55:04 - [INFO] Modulate Q, K, V at 0th layer.
2024/11/02 23:55:04 - [INFO] Modulate Q, K, V at 1th layer.
2024/11/02 23:55:04 - [INFO] Modulate Q, K, V at 2th layer.
2024/11/02 23:55:04 - [INFO] Modulate Q, K, V at 3th layer.
2024/11/02 23:55:04 - [INFO] Modulate Q, K, V at 4th layer.
2024/11/02 23:55:04 - [INFO] Modulate Q, K, V at 5th layer.
2024/11/02 23:55:04 - [INFO] Modulate Q, K, V at 6th layer.
2024/11/02 23:55:04 - [INFO] Modulate Q, K, V at 7th layer.
2024/11/02 23:55:04 - [INFO] Modulate Q, K, V at 8th layer.
2024/11/02 23:55:04 - [INFO] Complete the 8/10 step.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 0th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 1th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 2th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 3th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 4th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 5th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 6th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 7th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 8th layer.
2024/11/02 23:55:05 - [INFO] Complete the 9/10 step.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 0th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 1th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 2th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 3th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 4th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 5th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 6th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 7th layer.
2024/11/02 23:55:05 - [INFO] Modulate Q, K, V at 8th layer.
2024/11/02 23:55:05 - [INFO] Complete the 10/10 step.
